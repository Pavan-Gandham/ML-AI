{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIRf21zRpsN/H4aGLlW0fP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pavan-Gandham/ML-AI/blob/master/podcastToShortvideo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEQPAVz-MLSr"
      },
      "outputs": [],
      "source": [
        "pip install clipsai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install whisperx@git+https://github.com/m-bain/whisperx.git\n"
      ],
      "metadata": {
        "id": "R_SoNzPhMQSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ffmpeg-python"
      ],
      "metadata": {
        "id": "qGjWPJ-NMijm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-magic"
      ],
      "metadata": {
        "id": "Hqhn0obeM0mM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytube\n",
        "!pip install youtube-transcript-api"
      ],
      "metadata": {
        "id": "1Wl2XdQY-z6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "!pip install pytube\n",
        "!pip install opencv-python\n",
        "!pip install youtube-transcript-api"
      ],
      "metadata": {
        "id": "tLuv_Nlz7ZaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"AutoCropper_15May.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1NX3nKLD4ik3syVKj33B-Nl8aZUeOLG_A\n",
        "\n",
        "Cell 1: Installing necessary libraries\n",
        "\"\"\"\n",
        "\n",
        "# !pip install pytube\n",
        "# !pip install opencv-python\n",
        "# # !pip install openai\n",
        "# !pip install youtube-transcript-api\n",
        "\n",
        "\"\"\"Cell 2: Importing libraries and setting OpenAI API key\"\"\"\n",
        "\n",
        "from pytube import YouTube\n",
        "import cv2\n",
        "import subprocess\n",
        "# import openai\n",
        "import numpy as np\n",
        "import json\n",
        "import math\n",
        "import pdb\n",
        "\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "repo_id = 'google/flan-t5-xxl'\n",
        "# openai.api_key = ''  # Replace with your actual OpenAI API key\n",
        "HUGGING_FACE_HUB_API_KEY = \"\"\n",
        "\"\"\"Cell 3: Download YouTube Video function\"\"\"\n",
        "\n",
        "def download_video(url, filename):\n",
        "    yt = YouTube(url)\n",
        "    video = yt.streams.filter(file_extension='mp4').first()\n",
        "\n",
        "    # Download the video\n",
        "    video.download(filename=filename)\n",
        "\n",
        "\n",
        "#Segment Video function\n",
        "def segment_video(response):\n",
        "  for i, segment in enumerate(response):\n",
        "        start_time = math.floor(float(segment.get(\"start_time\", 0)))\n",
        "        end_time = math.ceil(float(segment.get(\"end_time\", 0))) + 2\n",
        "        output_file = f\"output{str(i).zfill(3)}.mp4\"\n",
        "        command = f\"ffmpeg -i input_video.mp4 -ss {start_time} -to {end_time} -c copy {output_file}\"\n",
        "        subprocess.call(command, shell=True)\n",
        "\n",
        "\n",
        "#Face Detection function\n",
        "def detect_faces(video_file):\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "    # Load the video\n",
        "    cap = cv2.VideoCapture(video_file)\n",
        "\n",
        "    faces = []\n",
        "\n",
        "    # Detect and store unique faces\n",
        "    while len(faces) < 5:\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            detected_faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "            # Iterate through the detected faces\n",
        "            for face in detected_faces:\n",
        "                # Check if the face is already in the list of faces\n",
        "                if not any(np.array_equal(face, f) for f in faces):\n",
        "                    faces.append(face)\n",
        "\n",
        "            # Print the number of unique faces detected so far\n",
        "            print(f\"Number of unique faces detected: {len(faces)}\")\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()\n",
        "\n",
        "    # If faces detected, return the list of faces\n",
        "    if len(faces) > 0:\n",
        "        return faces\n",
        "\n",
        "    # If no faces detected, return None\n",
        "    return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Crop Video function\n",
        "import cv2\n",
        "\n",
        "\n",
        "import cv2\n",
        "\n",
        "def crop_video(faces, input_file, output_file):\n",
        "    try:\n",
        "        if len(faces) > 0:\n",
        "            # Constants for cropping\n",
        "            CROP_RATIO = 0.9  # Adjust the ratio to control how much of the face is visible in the cropped video\n",
        "            VERTICAL_RATIO = 9 / 16  # Aspect ratio for the vertical video\n",
        "\n",
        "            # Read the input video\n",
        "            cap = cv2.VideoCapture(input_file)\n",
        "\n",
        "            # Get the frame dimensions\n",
        "            frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "            frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "            # Calculate the target width and height for cropping (vertical format)\n",
        "            target_height = int(frame_height * CROP_RATIO)\n",
        "            target_width = int(target_height * VERTICAL_RATIO)\n",
        "\n",
        "            # Create a VideoWriter object to save the output video\n",
        "            fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "            output_video = cv2.VideoWriter(output_file, fourcc, 30.0, (target_width, target_height))\n",
        "\n",
        "            # Loop through each frame of the input video\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "\n",
        "                # If no more frames, break out of the loop\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                # Iterate through each detected face\n",
        "                for face in faces:\n",
        "                    # Unpack the face coordinates\n",
        "                    x, y, w, h = face\n",
        "\n",
        "                    # Calculate the crop coordinates\n",
        "                    crop_x = max(0, x + (w - target_width) // 2)  # Adjust the crop region to center the face\n",
        "                    crop_y = max(0, y + (h - target_height) // 2)\n",
        "                    crop_x2 = min(crop_x + target_width, frame_width)\n",
        "                    crop_y2 = min(crop_y + target_height, frame_height)\n",
        "\n",
        "                    # Crop the frame based on the calculated crop coordinates\n",
        "                    cropped_frame = frame[crop_y:crop_y2, crop_x:crop_x2]\n",
        "\n",
        "                    # Resize the cropped frame to the target dimensions\n",
        "                    resized_frame = cv2.resize(cropped_frame, (target_width, target_height))\n",
        "\n",
        "                    # Write the resized frame to the output video\n",
        "                    output_video.write(resized_frame)\n",
        "\n",
        "            # Release the input and output video objects\n",
        "            cap.release()\n",
        "            output_video.release()\n",
        "\n",
        "            print(\"Video cropped successfully.\")\n",
        "        else:\n",
        "            print(\"No faces detected in the video.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during video cropping: {str(e)}\")\n",
        "\n",
        "\n",
        "def crop_video2(faces, input_file, output_file):\n",
        "    try:\n",
        "        if len(faces) > 0:\n",
        "            # Constants for cropping\n",
        "            CROP_RATIO = 0.9  # Adjust the ratio to control how much of the face is visible in the cropped video\n",
        "            VERTICAL_RATIO = 9 / 16  # Aspect ratio for the vertical video\n",
        "            BATCH_DURATION = 5  # Duration of each batch in seconds\n",
        "\n",
        "            # Read the input video\n",
        "            cap = cv2.VideoCapture(input_file)\n",
        "\n",
        "            # Get the frame dimensions\n",
        "            frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "            frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "            # Calculate the target width and height for cropping (vertical format)\n",
        "            target_height = int(frame_height * CROP_RATIO)\n",
        "            target_width = int(target_height * VERTICAL_RATIO)\n",
        "\n",
        "            # Calculate the number of frames per batch\n",
        "            frames_per_batch = int(cap.get(cv2.CAP_PROP_FPS) * BATCH_DURATION)\n",
        "\n",
        "            # Create a VideoWriter object to save the output video\n",
        "            fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "            output_video = cv2.VideoWriter(output_file, fourcc, 30.0, (target_width, target_height))\n",
        "\n",
        "            # Loop through each batch of frames\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "\n",
        "                # If no more frames, break out of the loop\n",
        "                if not ret:\n",
        "                    break\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # Convert frame to BGR color format\n",
        "                # Iterate through each detected face\n",
        "                for face in faces:\n",
        "                    # Unpack the face coordinates\n",
        "                    x, y, w, h = face\n",
        "\n",
        "                    # Calculate the crop coordinates\n",
        "                    crop_x = max(0, x + (w - target_width) // 2)  # Adjust the crop region to center the face\n",
        "                    crop_y = max(0, y + (h - target_height) // 2)\n",
        "                    crop_x2 = min(crop_x + target_width, frame_width)\n",
        "                    crop_y2 = min(crop_y + target_height, frame_height)\n",
        "\n",
        "                    # Crop the frame based on the calculated crop coordinates\n",
        "                    cropped_frame = frame[crop_y:crop_y2, crop_x:crop_x2]\n",
        "\n",
        "                    # Resize the cropped frame to the target dimensions\n",
        "                    resized_frame = cv2.resize(cropped_frame, (target_width, target_height))\n",
        "\n",
        "                    # Write the resized frame to the output video\n",
        "                    output_video.write(resized_frame)\n",
        "\n",
        "                    # Check if the current frame index is divisible by frames_per_batch\n",
        "                    if cap.get(cv2.CAP_PROP_POS_FRAMES) % frames_per_batch == 0:\n",
        "                        # Analyze the lip movement or facial muscle activity within the batch\n",
        "                        is_talking = is_talking_in_batch(resized_frame)\n",
        "\n",
        "                        # Adjust the focus based on the speaking activity\n",
        "                        adjust_focus(is_talking)\n",
        "\n",
        "            # Release the input and output video objects\n",
        "            cap.release()\n",
        "            output_video.release()\n",
        "\n",
        "            print(\"Video cropped successfully.\")\n",
        "        else:\n",
        "            print(\"No faces detected in the video.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during video cropping: {str(e)}\")\n",
        "\n",
        "def is_talking_in_batch(frames):\n",
        "    # Calculate the motion between consecutive frames\n",
        "    motion_scores = []\n",
        "    for i in range(len(frames) - 1):\n",
        "        frame1 = frames[i]\n",
        "        frame2 = frames[i+1]\n",
        "        motion_score = calculate_motion_score(frame1, frame2)  # Replace with your motion analysis function\n",
        "        motion_scores.append(motion_score)\n",
        "\n",
        "    # Determine if talking behavior is present based on motion scores\n",
        "    threshold = 0.5  # Adjust the threshold as needed\n",
        "    talking = any(score > threshold for score in motion_scores)\n",
        "\n",
        "    return talking\n",
        "\n",
        "def calculate_motion_score(frame1, frame2):\n",
        "    # Convert frames to grayscale\n",
        "    gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
        "    gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Calculate dense optical flow\n",
        "    flow = cv2.calcOpticalFlowFarneback(gray1, gray2, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "\n",
        "    # Calculate magnitude of optical flow vectors\n",
        "    magnitude = np.sqrt(flow[..., 0] ** 2 + flow[..., 1] ** 2)\n",
        "\n",
        "    # Calculate motion score as the average magnitude of optical flow vectors\n",
        "    motion_score = np.mean(magnitude)\n",
        "\n",
        "    return motion_score\n",
        "\n",
        "def adjust_focus(frame, talking):\n",
        "    if talking:\n",
        "        # Apply visual effects or adjustments to emphasize the speaker\n",
        "        # For example, you can add a bounding box or overlay text on the frame\n",
        "        # indicating the speaker is talking\n",
        "        # You can also experiment with resizing or positioning the frame to\n",
        "        # focus on the talking person\n",
        "\n",
        "        # Example: Draw a bounding box around the face region\n",
        "        face_coordinates = get_face_coordinates(frame)  # Replace with your face detection logic\n",
        "\n",
        "        if face_coordinates is not None:\n",
        "            x, y, w, h = face_coordinates\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "    return frame\n",
        "def get_face_coordinates(frame):\n",
        "    # Load the pre-trained Haar cascade classifier for face detection\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "    # Convert frame to grayscale for face detection\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces in the frame\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "    if len(faces) > 0:\n",
        "        # Return the coordinates of the first detected face\n",
        "        x, y, w, h = faces[0]\n",
        "        return x, y, w, h\n",
        "\n",
        "    # If no face detected, return None\n",
        "    return None\n",
        "\n",
        "def get_transcript(video_id):\n",
        "    # Get the transcript for the given YouTube video ID\n",
        "    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "\n",
        "    # Format the transcript for feeding into GPT-4\n",
        "    formatted_transcript = ''\n",
        "    for entry in transcript:\n",
        "        start_time = \"{:.2f}\".format(entry['start'])\n",
        "        end_time = \"{:.2f}\".format(entry['start'] + entry['duration'])\n",
        "        text = entry['text']\n",
        "        formatted_transcript += f\"{start_time} --> {end_time} : {text}\\n\"\n",
        "\n",
        "    return transcript\n",
        "\n",
        "\n",
        "\n",
        "#Analyze transcript with GPT-3 function\n",
        "response_obj='''[\n",
        "  {\n",
        "    \"start_time\": 97.19,\n",
        "    \"end_time\": 127.43,\n",
        "    \"description\": \"Spoken Text here\"\n",
        "    \"duration\":36 #Length in seconds\n",
        "  },\n",
        "  {\n",
        "    \"start_time\": 169.58,\n",
        "    \"end_time\": 199.10,\n",
        "    \"description\": \"Spoken Text here\"\n",
        "    \"duration\":33\n",
        "  },\n",
        "]'''\n",
        "def analyze_transcript(transcript):\n",
        "    prompt = f\"This is a transcript of a video. Please identify the 3 most viral sections from the whole, make sure they are more than 30 seconds in duration,Make Sure you provide extremely accurate timestamps respond only in this format {response_obj}  \\n Here is the Transcription:\\n{transcript}\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a ViralGPT helpful assistant. You are master at reading youtube transcripts and identifying the most Interesting and Viral Content\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=messages,\n",
        "        max_tokens=512,\n",
        "        n=1,\n",
        "        stop=None\n",
        "    )\n",
        "    return response.choices[0]['message']\n",
        "\n",
        "\"\"\"Main function and execution\"\"\"\n",
        "\n",
        "interseting_seg='''[{'text': 'happiness through Curiosity on Dr', 'start': 0.0, 'duration': 4.82}, {'text': 'eclipse', 'start': 2.28, 'duration': 2.54}, {'text': 'little rookie question for you okay and', 'start': 6.899, 'duration': 4.021}, {'text': \"I'm asking this on behalf of mainstream\", 'start': 9.24, 'duration': 3.6}, {'text': 'media how do you feel when you see', 'start': 10.92, 'duration': 5.4}, {'text': 'movies like pathan or tiger or any', 'start': 12.84, 'duration': 5.939}, {'text': \"Indian I think we haven't got the art of\", 'start': 16.32, 'duration': 4.5}, {'text': 'doing those movies you think they can be', 'start': 18.779, 'duration': 4.321}, {'text': 'done better oh yes I mean they can be', 'start': 20.82, 'duration': 3.42}, {'text': 'realistic', 'start': 23.1, 'duration': 3.12}, {'text': \"okay we're not realistic what you see\", 'start': 24.24, 'duration': 4.32}, {'text': 'what is not realistic about them huh', 'start': 26.22, 'duration': 4.219}, {'text': \"it's not realistic\", 'start': 28.56, 'duration': 4.38}, {'text': \"you're trying to make a James Bond movie\", 'start': 30.439, 'duration': 5.741}, {'text': 'which is also not realistic okay', 'start': 32.94, 'duration': 5.88}, {'text': 'then you have this story of the isi girl', 'start': 36.18, 'duration': 4.74}, {'text': 'in the raw man', 'start': 38.82, 'duration': 4.86}, {'text': 'living happily ever after I mean', 'start': 40.92, 'duration': 4.639}, {'text': 'take a break', 'start': 43.68, 'duration': 7.08}, {'text': 'has that ever happened not really right', 'start': 45.559, 'duration': 7.48}, {'text': 'no the whole atmospherics of the whole', 'start': 50.76, 'duration': 3.54}, {'text': 'thing you know', 'start': 53.039, 'duration': 3.36}, {'text': \"I haven't seen batana and I won't see it\", 'start': 54.3, 'duration': 5.099}, {'text': \"because I don't think it is an accurate\", 'start': 56.399, 'duration': 4.98}, {'text': \"depiction it's not an accurate I'm not\", 'start': 59.399, 'duration': 4.941}, {'text': 'going to waste my time', 'start': 61.379, 'duration': 2.961}, {'text': 'and I laughed and I enjoyed that because', 'start': 65.18, 'duration': 6.28}, {'text': 'it was so quaint', 'start': 68.04, 'duration': 5.7}, {'text': 'not because it was defeating anything', 'start': 71.46, 'duration': 3.659}, {'text': 'yeah', 'start': 73.74, 'duration': 5.4}, {'text': 'like you had that other movie of um', 'start': 75.119, 'duration': 7.5}, {'text': 'war that they can no this was this', 'start': 79.14, 'duration': 5.82}, {'text': 'fellow Salman Khan going under a tunnel', 'start': 82.619, 'duration': 5.281}, {'text': 'into Pakistan to deliver a girl who had', 'start': 84.96, 'duration': 4.88}, {'text': 'got legendary', 'start': 87.9, 'duration': 4.14}, {'text': 'but whatever', 'start': 89.84, 'duration': 4.86}, {'text': 'I mean', 'start': 92.04, 'duration': 2.66}, {'text': 'could I exaggerated okay this is not you', 'start': 95.46, 'duration': 5.4}, {'text': 'have to have entertainment which is fun', 'start': 99.0, 'duration': 4.079}, {'text': 'and realistic you should see that movie', 'start': 100.86, 'duration': 3.36}, {'text': 'The', 'start': 103.079, 'duration': 4.86}, {'text': 'Bridge of spies hey that is a real movie', 'start': 104.22, 'duration': 6.78}, {'text': 'okay that is how real spy movies are', 'start': 107.939, 'duration': 5.521}, {'text': 'made what does a real spy movie', 'start': 111.0, 'duration': 5.46}, {'text': 'constitute it means dealing with actual', 'start': 113.46, 'duration': 5.64}, {'text': 'facts no no blonde round no nothing', 'start': 116.46, 'duration': 4.74}, {'text': \"around it's okay living a lonely life\", 'start': 119.1, 'duration': 4.799}, {'text': \"you're living on by yourself living your\", 'start': 121.2, 'duration': 6.0}, {'text': 'cover story he able uh', 'start': 123.899, 'duration': 5.821}, {'text': 'with goldfish was actually a notice so', 'start': 127.2, 'duration': 3.839}, {'text': 'he was doing paintings he used to make', 'start': 129.72, 'duration': 3.78}, {'text': 'him make money out of it and but he was', 'start': 131.039, 'duration': 5.161}, {'text': 'doing this other job also so running is', 'start': 133.5, 'duration': 5.099}, {'text': 'espionage ring', 'start': 136.2, 'duration': 4.92}, {'text': 'and they show all that how a documents', 'start': 138.599, 'duration': 5.22}, {'text': 'are exchanged or document information is', 'start': 141.12, 'duration': 4.86}, {'text': 'exchanged you have things called letter', 'start': 143.819, 'duration': 5.941}, {'text': 'dead letter boxes a dead letter box in', 'start': 145.98, 'duration': 7.2}, {'text': 'in Espionage is a place it could be a', 'start': 149.76, 'duration': 6.42}, {'text': \"book let's say or or that statue I put\", 'start': 153.18, 'duration': 6.48}, {'text': 'my UBS under it', 'start': 156.18, 'duration': 5.279}, {'text': 'and leave it', 'start': 159.66, 'duration': 5.46}, {'text': 'and leave a sign outside on some tree or', 'start': 161.459, 'duration': 4.801}, {'text': 'a wall', 'start': 165.12, 'duration': 5.759}, {'text': \"that I've I've fed the the dead litter\", 'start': 166.26, 'duration': 6.42}, {'text': 'box okay so the other chap comes and', 'start': 170.879, 'duration': 3.661}, {'text': 'picks it up and takes it away the two', 'start': 172.68, 'duration': 4.26}, {'text': 'never meet based on the true nature of', 'start': 174.54, 'duration': 3.72}, {'text': 'espionage', 'start': 176.94, 'duration': 4.2}, {'text': \"which Indian actor's style would be best\", 'start': 178.26, 'duration': 7.259}, {'text': 'suited to portray the character of a spy', 'start': 181.14, 'duration': 6.84}, {'text': 'you know I I saw um', 'start': 185.519, 'duration': 4.921}, {'text': 'three three three actors were three or', 'start': 187.98, 'duration': 4.679}, {'text': 'four actors were very good this kind of', 'start': 190.44, 'duration': 3.299}, {'text': 'a thing', 'start': 192.659, 'duration': 3.901}, {'text': 'who could fit into these kind Sorrows', 'start': 193.739, 'duration': 6.481}, {'text': 'not giving any order of preference but', 'start': 196.56, 'duration': 7.02}, {'text': 'I like nawazuddin Siddiqui I used to', 'start': 200.22, 'duration': 5.599}, {'text': 'like Imran Khan', 'start': 203.58, 'duration': 4.439}, {'text': 'Irfan Khan sorry', 'start': 205.819, 'duration': 6.28}, {'text': 'and he was he was a consummate actor', 'start': 208.019, 'duration': 8.821}, {'text': 'Anup anupam care and', 'start': 212.099, 'duration': 8.241}, {'text': 'these two actors', 'start': 216.84, 'duration': 3.5}, {'text': 'the one who played family man um', 'start': 220.62, 'duration': 6.96}, {'text': 'very good okay they could fit into the', 'start': 224.84, 'duration': 8.02}, {'text': 'room and Mishra pankaj Mishra foreign', 'start': 227.58, 'duration': 5.28}, {'text': '[Music]', 'start': 233.72, 'duration': 3.11}, {'text': \"spy all right it's a cold war story\", 'start': 259.699, 'duration': 6.461}, {'text': \"about the the it's actually based on\", 'start': 263.52, 'duration': 5.179}, {'text': 'this Cambridge 5.', 'start': 266.16, 'duration': 5.9}, {'text': 'you know the Cambridge five those', 'start': 268.699, 'duration': 6.341}, {'text': 'Kim philby and others who were spying', 'start': 272.06, 'duration': 6.1}, {'text': 'for who were actually with the MI6 but', 'start': 275.04, 'duration': 6.0}, {'text': 'it was actually a KGB agent okay the', 'start': 278.16, 'duration': 5.58}, {'text': 'real mole and he would have been Chief', 'start': 281.04, 'duration': 4.08}, {'text': 'maybe one day', 'start': 283.74, 'duration': 4.08}, {'text': 'at the not been caught out', 'start': 285.12, 'duration': 7.579}, {'text': 'so on that is made a novel Tinker spy', 'start': 287.82, 'duration': 7.26}, {'text': \"it's beautifully done the book is\", 'start': 292.699, 'duration': 6.241}, {'text': 'marvelous and the acting and the', 'start': 295.08, 'duration': 3.86}, {'text': 'you should watch it okay and watch this', 'start': 302.78, 'duration': 6.04}, {'text': 'uh Bridge of spies if you enjoyed this', 'start': 305.88, 'duration': 5.9}, {'text': 'video subscribe TRS clips for more', 'start': 308.82, 'duration': 15.86}, {'text': '[Music]', 'start': 311.78, 'duration': 15.33}, {'text': 'thank you', 'start': 324.68, 'duration': 8.55}, {'text': '[Music]', 'start': 327.11, 'duration': 6.12}]''';\n",
        "\n",
        "def main():\n",
        "    video_id='B9XGUpQZY38'\n",
        "    url = 'https://www.youtube.com/watch?v='+video_id  # Replace with your video's URL\n",
        "    filename = 'input_video.mp4'\n",
        "    download_video(url,filename)\n",
        "\n",
        "    transcript = get_transcript(video_id)\n",
        "    print(transcript)\n",
        "    interesting_segment = analyze_transcript(transcript)\n",
        "    print(interesting_segment)\n",
        "    content = interesting_segment.message[\"content\"]\n",
        "    parsed_content = json.loads(content)\n",
        "    print(parsed_content)\n",
        "    #pdb.set_trace()\n",
        "    segment_video(parsed_content)\n",
        "\n",
        "    # Loop through each segment\n",
        "    for i in range(0, 3):  # Replace 3 with the actual number of segments\n",
        "        input_file = f'output{str(i).zfill(3)}.mp4'\n",
        "        output_file = f'output_cropped{str(i).zfill(3)}.mp4'\n",
        "        faces = detect_faces(input_file)\n",
        "        crop_video(faces, input_file, output_file)\n",
        "\n",
        "    # Assume you have a way to get the transcript. This is not shown here.\n",
        " # Replace with actual transcript\n",
        "\n",
        "\n",
        "# Run the main function\n",
        "main()"
      ],
      "metadata": {
        "id": "INQB4oPXQxj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import huggingface_hub\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from pytube import YouTube\n",
        "\n",
        "# repo_id = 'google/flan-t5-xxl'\n",
        "# HUGGING_FACE_HUB_API_KEY = \"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\"\n",
        "def download_video(url, filename):\n",
        "    yt = YouTube(url)\n",
        "    video = yt.streams.filter(file_extension='mp4').first()\n",
        "\n",
        "    # Download the video\n",
        "    video.download(filename=filename)\n",
        "\n",
        "def get_transcript(video_id):\n",
        "    # Get the transcript for the given YouTube video ID\n",
        "    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "\n",
        "    # Format the transcript for feeding into GPT-4\n",
        "    formatted_transcript = ''\n",
        "    for entry in transcript:\n",
        "        start_time = \"{:.2f}\".format(entry['start'])\n",
        "        end_time = \"{:.2f}\".format(entry['start'] + entry['duration'])\n",
        "        text = entry['text']\n",
        "        formatted_transcript += f\"{start_time} --> {end_time} : {text}\\n\"\n",
        "\n",
        "    return transcript\n",
        "\n",
        "\n",
        "# def analyze_transcript(transcript):\n",
        "#     response_obj='''[\n",
        "#   {\n",
        "#     \"start_time\": 97.19,\n",
        "#     \"end_time\": 127.43,\n",
        "#     \"description\": \"Spoken Text here\"\n",
        "#     \"duration\":36 #Length in seconds\n",
        "#   },\n",
        "#   {\n",
        "#     \"start_time\": 169.58,\n",
        "#     \"end_time\": 199.10,\n",
        "#     \"description\": \"Spoken Text here\"\n",
        "#     \"duration\":33\n",
        "#   },\n",
        "# ]'''\n",
        "#     prompt = f\"This is a transcript of a video. Please identify the 3 most viral sections from the whole, make sure they are more than 30 seconds in duration,Make Sure you provide extremely accurate timestamps respond only in this format {response_obj}  \\n Here is the Transcription:\\n{transcript}\"\n",
        "#     messages = [\n",
        "#         {\"role\": \"system\", \"content\": \"You are a ViralGPT helpful assistant. You are master at reading youtube transcripts and identifying the most Interesting and Viral Content\"},\n",
        "#         {\"role\": \"user\", \"content\": prompt}\n",
        "#     ]\n",
        "    # response = openai.ChatCompletion.create(\n",
        "    #     model=\"gpt-4\",\n",
        "    #     messages=messages,\n",
        "    #     max_tokens=512,\n",
        "    #     n=1,\n",
        "    #     stop=None\n",
        "    # )\n",
        "\n",
        "    # return response.choices[0]['message']\n",
        "    # return response\n",
        "    def main():\n",
        "      video_id='B9XGUpQZY38'\n",
        "      url = 'https://www.youtube.com/watch?v='+video_id  # Replace with your video's URL\n",
        "      filename = 'input_video.mp4'\n",
        "      download_video(url,filename)\n",
        "\n",
        "      transcript = get_transcript(video_id)\n",
        "      print(transcript)\n",
        "      # interesting_segment = analyze_transcript(transcript)\n",
        "      # print(interesting_segment)\n",
        "main()"
      ],
      "metadata": {
        "id": "GixyM7AAU9xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5IEbJfsD94rn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}